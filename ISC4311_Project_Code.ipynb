{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0e7dcf-080f-4aac-adc5-decb0d4bca12",
   "metadata": {},
   "source": [
    "## **Classification of Rice From Images Using CNNs**\n",
    "### ISC4311 Final Project | Rhea Nibert | December 4, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e6978-97e9-455b-a0fa-9b0ad4909b45",
   "metadata": {},
   "source": [
    "### **Introduction to Notebook**\n",
    "\n",
    "This is the notebook for the entire workflow on my course project. The goal of this project is to process images of rice grains and classify them into one of five types (Arborio, Basmati, Ipsala, Jasmine, or Karacadag) using supervised learning, specifically convolutional neural networks (CNNs). \n",
    "\n",
    "**IMPORTANT**:\n",
    "- The code was developed with Python 3.11 to ensure support for TensorFlow. If you run into issues, try downgrading your Python environment version.\n",
    "- The cells are meant to be run sequentially from top to bottom, so if rerunning code, be sure to use the \"Run All Cells\" option.\n",
    "- This notebook will download large files to your device's storage, but don't worry about deleting them, as I handle this step in the code.\n",
    "- I sampled just 5000 images from this folder for training my model, which your CPU should be able to handle, but you can try using a smaller `random.sample()` if you run into memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716ff13-c122-416a-bd0e-d2a4b5ae8633",
   "metadata": {},
   "source": [
    "I began my workflow by importing all Python packages that I will use. If running the notebook for the first time, uncomment the first line to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b740c9-6ccc-4211-ab53-c2e1aefaa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import tempfile\n",
    "import requests\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Seed random generators\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d006b-369c-4b77-b1b4-9a344462a792",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**\n",
    "\n",
    "The first step is loading in the dataset. I used the `kagglehub` library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a4e199d-c894-4233-ad42-d85a5340f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/muratkokludataset/rice-image-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 219M/219M [00:07<00:00, 32.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /Users/rheanibert/.cache/kagglehub/datasets/muratkokludataset/rice-image-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"muratkokludataset/rice-image-dataset\")\n",
    "\n",
    "# Print path\n",
    "print(f\"Dataset downloaded to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90eb14-9151-4c5e-bbef-6df519e34e36",
   "metadata": {},
   "source": [
    "Upon inspecting this folder in local storage, I discovered that the images for each rice type are stored in subfolders. I walked through the folders in the directory to locate the paths of these target folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fde4602-ea07-4d24-817c-b06af3d04fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths of folders containing images:\n"
     ]
    }
   ],
   "source": [
    "# Define name of each rice type (folder names)\n",
    "rice_types = [\"Arborio\", \"Basmati\", \"Ipsala\", \"Jasmine\", \"Karacadag\"]\n",
    "\n",
    "# Walk through the dataset directory and find these folders\n",
    "folder_paths = []\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for d in dirs:\n",
    "        if d in rice_types:\n",
    "            full_path = os.path.join(root, d)\n",
    "            folder_paths.append(full_path)\n",
    "\n",
    "# Print folder paths\n",
    "print(\"Paths of folders containing images:\")\n",
    "for path in folder_paths:\n",
    "    print(f\"{path.split('/')[-1]}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6fdca-9a83-4b4f-b7b7-309d1cfaa59c",
   "metadata": {},
   "source": [
    "Each image needs to be processed so it works with CNNs. I defined the function below to process the images. I set the size to 224 because I found that this is the standard size for images in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c33fc95-92a4-4ea1-9458-cce62f0d79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process images\n",
    "def process_image(img_file, size=224):\n",
    "    # Convert the color channels to RBG\n",
    "    img = Image.open(img_file).convert(\"RGB\")\n",
    "    \n",
    "    # Resize the image\n",
    "    img = img.resize((size, size))\n",
    "    \n",
    "    # Normalize the color channels to 0-1\n",
    "    img = np.array(img) / 255.0 \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df600df-128c-492b-bc21-c2cd91078e3e",
   "metadata": {},
   "source": [
    "I looped through the image folders and rice type labels to form the NumPy arrays used for modeling. I sampled 1000 images of each rice type because there were originally 15000 each, which is too computationally expensive. I ran the `process_image` function on each image from the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f52289-d5e6-4156-bf94-b7446d077011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 0 images total\n"
     ]
    }
   ],
   "source": [
    "# Define lists to save images and class labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each rice type and the folder path with its images\n",
    "for rice_type, folder in zip(rice_types, folder_paths):\n",
    "    # Convert the folder string to a path\n",
    "    folder = Path(folder)\n",
    "\n",
    "    # Get all images from the folder\n",
    "    all_images = list(folder.glob(\"*\"))\n",
    "\n",
    "    # Sample 1000 images from the folder\n",
    "    sampled_images = random.sample(all_images, 1000)\n",
    "\n",
    "    # Loop through the image files\n",
    "    for image_file in sampled_images: \n",
    "        # Process the image\n",
    "        image = process_image(image_file)\n",
    "\n",
    "        # Add images and labels to lists\n",
    "        images.append(image)\n",
    "        labels.append(rice_type)\n",
    "             \n",
    "    print(f\"Loaded {rice_type} images\") \n",
    "\n",
    "# Convert images and labels to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Generate a random permutation of indices\n",
    "idx = np.random.permutation(len(images))\n",
    "\n",
    "# Shuffle both arrays using this index order\n",
    "images = images[idx]\n",
    "labels = labels[idx]\n",
    "\n",
    "# Confirm number of images loaded\n",
    "print(f\"\\nLoaded {len(images)} images total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee93cdc-b9bd-4abc-8174-8a4ba47ccabd",
   "metadata": {},
   "source": [
    "To ensure the images were processed correctly, I printed a random image of each rice type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06094b00-8e92-48b1-9515-2a672be2eed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m indices = np.where(labels == label)[\u001b[32m0\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Pick one random index\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m random_idx = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Get the corresponding image\u001b[39;00m\n\u001b[32m     12\u001b[39m img = images[random_idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:951\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: 'a' cannot be empty unless no samples are taken"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, len(rice_types), figsize=(15, 5))\n",
    "\n",
    "for i, label in enumerate(rice_types):\n",
    "    # Find all indices for this specific label\n",
    "    indices = np.where(labels == label)[0]\n",
    "    \n",
    "    # Pick one random index\n",
    "    random_idx = np.random.choice(indices)\n",
    "   \n",
    "    # Get the corresponding image\n",
    "    img = images[random_idx]\n",
    "    \n",
    "    # Display it in the subplot\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150a298-443c-4e4e-b2bc-b54a44e03f84",
   "metadata": {},
   "source": [
    "The final step before modeling is to encode the string labels as numbers and perform a train/validation/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d398b-9166-49b5-bfe7-7291c251e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as integers and ensure they are treated as categorical (one-hot encoding)\n",
    "le = LabelEncoder()\n",
    "labels_int = le.fit_transform(labels)\n",
    "labels_one_hot = to_categorical(labels_int, num_classes=5)\n",
    "\n",
    "# Split into Train (70%), Validation (15%), Test (15%)\n",
    "# First split: 70% Train, 30% Temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    images, labels_one_hot, test_size=0.3, random_state=42, \n",
    "    stratify=labels_int # Ensure class balance\n",
    ")\n",
    "\n",
    "# Second split: Split the 30% Temp into equal Validation and Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, \n",
    "    stratify=np.argmax(y_temp, axis=1) # Ensure class balance (reverse one-hot-encoding for this)\n",
    ")\n",
    "\n",
    "print(f\"Training shape: {X_train.shape}\")\n",
    "print(f\"Validation shape: {X_val.shape}\")\n",
    "print(f\"Testing shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6ee23-397e-4478-a569-ccee0c097888",
   "metadata": {},
   "source": [
    "I then proceeded to create my CNN model. I used a common deep learning structure for learning patterns from images:\n",
    "\n",
    "1. **Input Layer**\n",
    "\n",
    "    The model accepts images with a shape of 224×224×3 (RGB). This ensures all images entering the network have a consistent size.\n",
    "\n",
    "2. **Convolution + Pooling Feature Extraction Blocks**\n",
    "\n",
    "    The first part of the model consists of three convolutional blocks. Each block contains:\n",
    "\n",
    "    * `Conv2D` layers to learn visual features\n",
    "      (starting from simple edges → textures → complex shapes)\n",
    "    * `BatchNormalization` to stabilize and speed up training\n",
    "    * `MaxPooling2D` to reduce spatial dimensions and prevent overfitting\n",
    "\n",
    "    As the blocks progress, the number of filters increases (32 → 64 → 128) to allow the network to learn richer patterns.\n",
    "\n",
    "    A small `Dropout` layer is added after the final block to reduce overfitting.\n",
    "\n",
    "3. **Dense Classification**\n",
    "\n",
    "    After feature extraction, the data is flattened and passed into fully connected layers:\n",
    "\n",
    "    * A `Dense` hidden layer learns relationships between the extracted features\n",
    "    * `BatchNormalization` + `Dropout` help generalize and prevent the model from memorizing the data\n",
    "    * A final `Dense` layer with softmax activation outputs probabilities across the 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36651306-4e6e-4df2-a789-b2c23418692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        # Input dimension later\n",
    "        Input(shape=(224, 224, 3)),\n",
    "\n",
    "        # Block 1: Capture basic features\n",
    "        Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # Block 2: Capture textures and patterns\n",
    "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # Block 3: Capture complex shapes\n",
    "        Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25), \n",
    "\n",
    "        # Flattening and Dense Layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5), \n",
    "\n",
    "        # Output Layer\n",
    "        Dense(5, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and summarize\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ecd3c-b7e8-4a9c-b2ff-b6196e63f5e1",
   "metadata": {},
   "source": [
    "After defining the model, I compiled it using the Adam optimizer and categorical cross-entropy loss, which is appropriate for multi-class classification tasks with one-hot encoded labels. I also tracked accuracy during training to evaluate model performance.\n",
    "To improve training efficiency and prevent overfitting, I set up two callbacks:\n",
    "1. `EarlyStopping` monitors the validation loss and stops training if it doesn’t improve for 5 consecutive epochs, restoring the model weights from the epoch with the best performance. This prevents unnecessary training once the model has converged.\n",
    "2. `ReduceLROnPlateau` also monitors the validation loss and reduces the learning rate by a factor of 0.1 if progress stalls for 3 epochs, allowing the model to fine-tune its learning and potentially escape small plateaus.\n",
    "   \n",
    "These callbacks ensure that training adapts dynamically, helping the model converge faster, maintain the best weights, and avoid wasting computation on redundant epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134cc6f1-af90-42cf-b671-24bec25805a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=\"adam\",                  # Adjusts learning rate dynamically\n",
    "    loss=\"categorical_crossentropy\",   # Multi-class classification with one-hot encoded labels\n",
    "    metrics=[\"accuracy\"]               # Evaluate based on accuracy of predictions\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",              # Watches validation loss\n",
    "    patience=5,                      # Stop training if val loss doesn't improve after 5 epochs\n",
    "    restore_best_weights=True,       # Restore model from the best epoch\n",
    "    verbose=1                        # Print a message when early stopping happens\n",
    ")\n",
    "\n",
    "# Reduce learning rate callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",              # Watches validation loss\n",
    "    factor=0.1,                      # Learning rate x 0.1\n",
    "    patience=3,                      # Reduce learning rate if no improvement after 3 epochs\n",
    "    min_lr=1e-6,                     # Ensure learning rate doesn't get too small\n",
    "    verbose=1                        # Print when learning rate is reduced\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e52ea-cbd9-4501-8baa-fc150bbf0acc",
   "metadata": {},
   "source": [
    "Training the model takes a while, so I saved the output and trained model to avoid having to run the training process again to access the model information. In the below cell, I defined a class to write output to the console and a file when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e66904-5f1d-4943-af47-25bb626bf2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleLog(object):\n",
    "    def __init__(self, filename):\n",
    "        self.file = open(filename, \"w\")\n",
    "        self.stdout = sys.stdout\n",
    "\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.stdout.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "        self.stdout.flush()\n",
    "        \n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "        sys.stdout = self.stdout "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233de65-86e4-40ed-9ee6-362baf42fff3",
   "metadata": {},
   "source": [
    "I then trained the model and set up loggers to capture the training process and save the final model. Note that the code below simply loads in the pre-created files, but you can uncomment the training code to train the model in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe74160-199d-4130-b811-337b2f140305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save printed training logs and print to console\n",
    "sys.stdout = DoubleLog(\"training_console_output.txt\")\n",
    "\n",
    "# CSV logger saves per-epoch results\n",
    "csv_logger = CSVLogger(\"training_metrics.csv\", append=False)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,                       # Max number of epochs\n",
    "    batch_size=32,                   # Number of samples processed at a time\n",
    "    validation_data=(X_val, y_val),  # Use validation data during training\n",
    "    callbacks=[early_stopping, reduce_lr, csv_logger], # Add callbacks\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"cnn_model_keras.keras\")\n",
    "\n",
    "# Close the logger\n",
    "sys.stdout.close()\n",
    "\"\"\"\n",
    "\n",
    "# GitHub URLs containing trained model data\n",
    "console_output_url = \"https://github.com/rheanibert04/CNN_Rice_Classification/raw/refs/heads/main/training_console_output.txt\"\n",
    "metrics_url = \"https://github.com/rheanibert04/CNN_Rice_Classification/raw/refs/heads/main/training_metrics.csv\"\n",
    "model_url = \"https://github.com/rheanibert04/CNN_Rice_Classification/raw/refs/heads/main/cnn_model_keras.keras\"\n",
    "\n",
    "# Download and print training output\n",
    "response = requests.get(console_output_url)\n",
    "console_text = response.text\n",
    "print(\"Training Output\")\n",
    "print(console_text)\n",
    "\n",
    "# Load training metrics for later use\n",
    "metrics = pd.read_csv(metrics_url)\n",
    "\n",
    "# Download the model file\n",
    "local_file = \"cnn_model_keras.keras\"\n",
    "response = requests.get(model_url)\n",
    "with open(local_file, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Load the model\n",
    "model = load_model(local_file, compile=False)\n",
    "\n",
    "# Delete the local model file\n",
    "os.remove(local_file)\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=\"adam\",                 \n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]               \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51e924-73da-4365-bb5f-44ea72b10992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Plot Training vs Validation Accuracy and Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(metrics[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(metrics[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(metrics[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(metrics[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 2. Evaluate on Test Data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ccf72-ce5f-4015-bf8d-10631cff1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get model predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions from probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Plot it\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fee87-bd9d-46f4-80c2-b2c2d58755fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9d324-aa73-40db-9ef7-e025d0305a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
